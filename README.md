Ce projet de Data Engineering proche des standards industriels consiste Ã  ingÃ©rer des donnÃ©es publiques sur les sÃ©ismes Ã  partir des flux GeoJSON de lâ€™USGS, Ã  les stocker dans une base de donnÃ©es PostgreSQL, Ã  les transformer en donnÃ©es analytiques exploitables, puis Ã  orchestrer lâ€™ensemble du pipeline avec Apache Airflow, le tout dans un environnement dockerisÃ©.

ğŸ“Š Jeu de donnÃ©es

USGS Earthquake Hazards Program â€” Flux GeoJSON publics (sans clÃ© API)

Ces flux fournissent des informations en quasi temps rÃ©el sur les sÃ©ismes :

magnitude

localisation

date et heure

profondeur

coordonnÃ©es gÃ©ographiques

ğŸ”— Source officielle :
https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php

ğŸ‘‰ ConcrÃ¨tement : on rÃ©cupÃ¨re automatiquement les sÃ©ismes survenus dans le monde (par exemple sur les derniÃ¨res 24 heures).

ğŸ—ï¸ Architecture du projet (Vue dâ€™ensemble)
1ï¸âƒ£ Extraction (Extract)

Ce que lâ€™on fait
On interroge lâ€™API publique de lâ€™USGS pour rÃ©cupÃ©rer les donnÃ©es de sÃ©ismes au format GeoJSON (flux des derniÃ¨res 24 heures).

ğŸ‘‰ Pourquoi ?
Câ€™est une situation rÃ©aliste de rÃ©cupÃ©ration de donnÃ©es externes via API, trÃ¨s courante en entreprise.

2ï¸âƒ£ Chargement â€“ couche brute (Load / Raw)

Ce que lâ€™on fait
Chaque Ã©vÃ©nement sismique est stockÃ© tel quel dans PostgreSQL, dans une table appelÃ©e :

raw.usgs_earthquakes


Les donnÃ©es sont stockÃ©es en JSONB

Chaque sÃ©isme a un identifiant unique

ğŸ‘‰ Pourquoi ?

Conserver la donnÃ©e originale sans perte

Permettre un retraitement ultÃ©rieur

Assurer la traÃ§abilitÃ© des donnÃ©es

3ï¸âƒ£ Transformation â€“ couche analytique (Transform / Curated)

Ce que lâ€™on fait
On extrait les champs utiles du JSON (date, magnitude, lieu, coordonnÃ©esâ€¦) pour crÃ©er une table structurÃ©e :

curated.earthquakes


ğŸ‘‰ Pourquoi ?

Les donnÃ©es sont plus faciles Ã  interroger en SQL

Meilleures performances

DonnÃ©es prÃªtes pour lâ€™analyse, la BI ou le machine learning

4ï¸âƒ£ ContrÃ´les de qualitÃ© (Data Quality Checks)

Ce que lâ€™on fait
On vÃ©rifie automatiquement :

quâ€™un identifiant de sÃ©isme existe

que les magnitudes sont dans des valeurs cohÃ©rentes

que les coordonnÃ©es gÃ©ographiques sont prÃ©sentes

ğŸ‘‰ Pourquoi ?

Ã‰viter des donnÃ©es corrompues

Garantir la fiabilitÃ© du pipeline

5ï¸âƒ£ Orchestration avec Apache Airflow

Ce que lâ€™on fait
Un DAG Airflow planifie et enchaÃ®ne automatiquement :

lâ€™extraction

le chargement

la transformation

les contrÃ´les de qualitÃ©

ğŸ‘‰ Pourquoi ?

Automatisation complÃ¨te

Supervision visuelle

Relance facile en cas dâ€™erreur

ğŸ§° Technologies utilisÃ©es

Python : ingestion API, transformation des donnÃ©es

PostgreSQL : stockage des donnÃ©es (raw + curated)

Apache Airflow : orchestration et planification

Docker Compose : environnement reproductible

ğŸ‘‰ En clair : nâ€™importe qui peut lancer le projet avec une seule commande.